{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: The Mouse in the Maze\n",
    "## Understanding Sequential Processing with RNNs\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how RNNs process sequences step-by-step\n",
    "- Visualize hidden state evolution through time\n",
    "- Observe how memory degrades over long sequences\n",
    "- Compare simple RNN vs LSTM performance\n",
    "- Establish baseline for transformer comparison\n",
    "\n",
    "**Prerequisites:** Basic understanding of neural networks\n",
    "\n",
    "**Estimated Time:** 45-60 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metaphor: The Mouse in the Maze\n",
    "\n",
    "Imagine you're a mouse trying to navigate a maze to find cheese:\n",
    "\n",
    "- üê≠ **You can only see your immediate surroundings** - no bird's-eye view\n",
    "- üß† **You must remember where you've been** - but your memory is limited\n",
    "- üö∂ **You must process the maze step-by-step** - you can't teleport\n",
    "- üìâ **The longer the path, the hazier your memory** - early turns fade away\n",
    "\n",
    "This is exactly how RNNs work: **sequential processing with limited memory**.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Before transformers revolutionized AI, this sequential constraint was fundamental. Understanding *why* it's limiting will help you appreciate what transformers achieve.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Our Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Our modules\n",
    "from maze_envs import generate_simple_maze, MazeDataset, MazeConfig, Maze\n",
    "from visualizations import MazeVisualizer, TrainingVisualizer, set_style\n",
    "from rnn_solver import create_simple_rnn, create_lstm, RNNTrainer, RNNMazeSolver\n",
    "\n",
    "# Set consistent style\n",
    "set_style()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Problem\n",
    "\n",
    "### 1.1 Generate a Sample Maze\n",
    "\n",
    "Let's create a simple maze and see what the \"mouse\" needs to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a maze\n",
    "maze = generate_simple_maze(size=15, seed=42)\n",
    "solution = maze.solve()\n",
    "\n",
    "print(f\"Maze size: {maze.config.height}x{maze.config.width}\")\n",
    "print(f\"Start: {maze.start}\")\n",
    "print(f\"Goal: {maze.goal}\")\n",
    "print(f\"Solution length: {len(solution)} steps\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "MazeVisualizer.plot_maze(maze, ax=axes[0], title=\"Unsolved Maze\")\n",
    "MazeVisualizer.plot_maze(maze, ax=axes[1], show_solution=True, title=\"Optimal Solution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal path (first 10 moves): {maze.path_to_actions(solution)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Sequential Challenge\n",
    "\n",
    "The path above has ~{solution_length} steps. An RNN must:\n",
    "\n",
    "1. **Start at S** with an initial hidden state\n",
    "2. **Process each position** one at a time\n",
    "3. **Update hidden state** after each step (trying to remember history)\n",
    "4. **Predict the next move** based only on current position + hidden state\n",
    "5. **Repeat** until reaching G\n",
    "\n",
    "The critical question: **Can the hidden state remember the path from 20 steps ago?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Math Behind RNNs\n",
    "\n",
    "### 2.1 Simple RNN: The Basic Mouse\n",
    "\n",
    "The fundamental RNN equation:\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "\n",
    "where:\n",
    "- $h_t$ = hidden state at time $t$ (the \"memory\")\n",
    "- $h_{t-1}$ = previous hidden state (what we remember)\n",
    "- $x_t$ = current input (where we are now)\n",
    "- $W_{hh}, W_{xh}, b_h$ = learned parameters\n",
    "\n",
    "**Key insight**: $h_t$ depends ONLY on $h_{t-1}$ and $x_t$. To remember step 1 at step 20, information must flow through 19 intermediate states!\n",
    "\n",
    "### 2.2 LSTM: The Mouse with Better Memory\n",
    "\n",
    "LSTM adds memory \"gates\" to combat vanishing gradients:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f) \\quad \\text{(forget gate)} \\\\\n",
    "i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i) \\quad \\text{(input gate)} \\\\\n",
    "o_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o) \\quad \\text{(output gate)} \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_C [h_{t-1}, x_t] + b_C) \\quad \\text{(candidate cell)} \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\quad \\text{(cell state)} \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t) \\quad \\text{(hidden state)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Key insight**: The cell state $C_t$ provides a \"highway\" for gradients, helping memory persist longer.\n",
    "\n",
    "But even LSTMs struggle with very long sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Training Data Preparation\n",
    "\n",
    "### 3.1 Generate Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "maze_config = MazeConfig(\n",
    "    height=15,\n",
    "    width=15,\n",
    "    wall_probability=0.25,\n",
    "    ensure_solvable=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating training data...\")\n",
    "train_dataset = MazeDataset(num_mazes=200, config=maze_config)\n",
    "\n",
    "maze_config.seed = 1000\n",
    "val_dataset = MazeDataset(num_mazes=50, config=maze_config)\n",
    "\n",
    "print(f\"‚úì Training mazes: {len(train_dataset)}\")\n",
    "print(f\"‚úì Validation mazes: {len(val_dataset)}\")\n",
    "\n",
    "# Analyze path lengths\n",
    "train_lengths = [len(sol) for _, sol in train_dataset]\n",
    "val_lengths = [len(sol) for _, sol in val_dataset]\n",
    "\n",
    "print(f\"\\nPath length statistics:\")\n",
    "print(f\"  Training: {np.mean(train_lengths):.1f} ¬± {np.std(train_lengths):.1f} steps\")\n",
    "print(f\"  Range: [{min(train_lengths)}, {max(train_lengths)}]\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(train_lengths, bins=20, alpha=0.7, label='Training', edgecolor='black')\n",
    "plt.hist(val_lengths, bins=20, alpha=0.7, label='Validation', edgecolor='black')\n",
    "plt.xlabel('Path Length (steps)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Solution Path Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare Training Batches\n",
    "\n",
    "We need to convert mazes into tensor format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(dataset, indices, device):\n",
    "    \"\"\"\n",
    "    Prepare a batch of mazes and solutions for training.\n",
    "    \n",
    "    Returns:\n",
    "        maze_grids: [batch_size, height, width]\n",
    "        position_sequences: [batch_size, max_len, 2]\n",
    "        action_sequences: [batch_size, max_len]\n",
    "    \"\"\"\n",
    "    batch_mazes = [dataset.mazes[i] for i in indices]\n",
    "    batch_solutions = [dataset.solutions[i] for i in indices]\n",
    "    \n",
    "    # Find max sequence length in batch\n",
    "    max_len = max(len(sol) for sol in batch_solutions)\n",
    "    \n",
    "    maze_grids = []\n",
    "    position_seqs = []\n",
    "    action_seqs = []\n",
    "    \n",
    "    for maze, solution in zip(batch_mazes, batch_solutions):\n",
    "        # Maze grid\n",
    "        maze_grids.append(maze.grid)\n",
    "        \n",
    "        # Position sequence (padded)\n",
    "        positions = np.array(solution)\n",
    "        pad_len = max_len - len(positions)\n",
    "        if pad_len > 0:\n",
    "            positions = np.vstack([positions, np.zeros((pad_len, 2), dtype=int)])\n",
    "        position_seqs.append(positions)\n",
    "        \n",
    "        # Action sequence (padded with -1 for ignore)\n",
    "        actions = maze.path_to_actions(solution)\n",
    "        action_map = {'UP': 0, 'DOWN': 1, 'LEFT': 2, 'RIGHT': 3}\n",
    "        action_ids = [action_map[a] for a in actions]\n",
    "        \n",
    "        # Pad with -1 (will be ignored in loss)\n",
    "        action_ids.extend([-1] * (max_len - 1 - len(action_ids)))\n",
    "        action_seqs.append(action_ids)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    maze_grids = torch.tensor(np.stack(maze_grids), dtype=torch.long, device=device)\n",
    "    position_seqs = torch.tensor(np.stack(position_seqs), dtype=torch.long, device=device)\n",
    "    action_seqs = torch.tensor(np.stack(action_seqs), dtype=torch.long, device=device)\n",
    "    \n",
    "    return maze_grids, position_seqs, action_seqs\n",
    "\n",
    "# Test the function\n",
    "test_batch = prepare_batch(train_dataset, [0, 1, 2], device)\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  Maze grids: {test_batch[0].shape}\")\n",
    "print(f\"  Positions: {test_batch[1].shape}\")\n",
    "print(f\"  Actions: {test_batch[2].shape}\")\n",
    "print(f\"\\n‚úì Data preparation working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Train Simple RNN\n",
    "\n",
    "### 4.1 Create and Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple RNN\n",
    "simple_rnn = create_simple_rnn(maze_size=15, hidden_dim=128)\n",
    "simple_rnn = simple_rnn.to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(simple_rnn)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in simple_rnn.parameters()):,}\")\n",
    "\n",
    "# Create trainer\n",
    "rnn_trainer = RNNTrainer(simple_rnn, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# History tracking\n",
    "rnn_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"Training Simple RNN...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    simple_rnn.train()\n",
    "    \n",
    "    # Shuffle training data\n",
    "    train_indices = np.random.permutation(len(train_dataset))\n",
    "    \n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        batch_idx = train_indices[i:i+batch_size]\n",
    "        maze_grids, positions, actions = prepare_batch(train_dataset, batch_idx, device)\n",
    "        \n",
    "        # Remove last position (no action to predict)\n",
    "        positions = positions[:, :-1, :]\n",
    "        \n",
    "        loss = rnn_trainer.train_step(maze_grids, positions, actions)\n",
    "        train_losses.append(loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        batch_idx = list(range(i, min(i+batch_size, len(val_dataset))))\n",
    "        maze_grids, positions, actions = prepare_batch(val_dataset, batch_idx, device)\n",
    "        positions = positions[:, :-1, :]\n",
    "        \n",
    "        val_loss, val_acc = rnn_trainer.evaluate(maze_grids, positions, actions)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    \n",
    "    # Record history\n",
    "    rnn_history['train_loss'].append(np.mean(train_losses))\n",
    "    rnn_history['val_loss'].append(np.mean(val_losses))\n",
    "    rnn_history['val_acc'].append(np.mean(val_accs))\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {rnn_history['train_loss'][-1]:.4f} | \"\n",
    "              f\"Val Loss: {rnn_history['val_loss'][-1]:.4f} | \"\n",
    "              f\"Val Acc: {rnn_history['val_acc'][-1]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = TrainingVisualizer.plot_training_curves(rnn_history, figsize=(15, 5))\n",
    "fig.suptitle('Simple RNN Training Progress', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Train LSTM (Better Memory)\n",
    "\n",
    "### 5.1 Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM\n",
    "lstm_model = create_lstm(maze_size=15, hidden_dim=128)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "print(f\"LSTM parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n",
    "print(f\"Simple RNN parameters: {sum(p.numel() for p in simple_rnn.parameters()):,}\")\n",
    "print(f\"\\nLSTM is ~{sum(p.numel() for p in lstm_model.parameters()) / sum(p.numel() for p in simple_rnn.parameters()):.1f}x larger (gates require more parameters)\")\n",
    "\n",
    "# Create trainer\n",
    "lstm_trainer = RNNTrainer(lstm_model, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# History tracking\n",
    "lstm_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"Training LSTM...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    lstm_model.train()\n",
    "    \n",
    "    train_indices = np.random.permutation(len(train_dataset))\n",
    "    \n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        batch_idx = train_indices[i:i+batch_size]\n",
    "        maze_grids, positions, actions = prepare_batch(train_dataset, batch_idx, device)\n",
    "        positions = positions[:, :-1, :]\n",
    "        \n",
    "        loss = lstm_trainer.train_step(maze_grids, positions, actions)\n",
    "        train_losses.append(loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        batch_idx = list(range(i, min(i+batch_size, len(val_dataset))))\n",
    "        maze_grids, positions, actions = prepare_batch(val_dataset, batch_idx, device)\n",
    "        positions = positions[:, :-1, :]\n",
    "        \n",
    "        val_loss, val_acc = lstm_trainer.evaluate(maze_grids, positions, actions)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    \n",
    "    # Record history\n",
    "    lstm_history['train_loss'].append(np.mean(train_losses))\n",
    "    lstm_history['val_loss'].append(np.mean(val_losses))\n",
    "    lstm_history['val_acc'].append(np.mean(val_accs))\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {lstm_history['train_loss'][-1]:.4f} | \"\n",
    "              f\"Val Loss: {lstm_history['val_loss'][-1]:.4f} | \"\n",
    "              f\"Val Acc: {lstm_history['val_acc'][-1]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compare RNN vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(rnn_history['val_loss'], label='Simple RNN', linewidth=2, marker='o')\n",
    "axes[0].plot(lstm_history['val_loss'], label='LSTM', linewidth=2, marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].set_title('Validation Loss: RNN vs LSTM', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].plot(rnn_history['val_acc'], label='Simple RNN', linewidth=2, marker='o')\n",
    "axes[1].plot(lstm_history['val_acc'], label='LSTM', linewidth=2, marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Accuracy')\n",
    "axes[1].set_title('Validation Accuracy: RNN vs LSTM', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Simple RNN - Val Loss: {rnn_history['val_loss'][-1]:.4f}, Val Acc: {rnn_history['val_acc'][-1]:.3f}\")\n",
    "print(f\"  LSTM       - Val Loss: {lstm_history['val_loss'][-1]:.4f}, Val Acc: {lstm_history['val_acc'][-1]:.3f}\")\n",
    "print(f\"\\n  LSTM is {((lstm_history['val_acc'][-1] - rnn_history['val_acc'][-1]) / rnn_history['val_acc'][-1] * 100):.1f}% better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Test on Example Mazes\n",
    "\n",
    "### 6.1 Generate Test Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test maze\n",
    "test_maze = generate_simple_maze(size=15, seed=999)\n",
    "optimal_solution = test_maze.solve()\n",
    "\n",
    "print(f\"Test maze difficulty: {len(optimal_solution)} steps\")\n",
    "\n",
    "# Generate paths from both models\n",
    "print(\"\\nGenerating paths...\")\n",
    "rnn_path = simple_rnn.generate_path(\n",
    "    torch.tensor(test_maze.grid, dtype=torch.long).unsqueeze(0).to(device),\n",
    "    test_maze.start,\n",
    "    max_steps=100\n",
    ")\n",
    "\n",
    "lstm_path = lstm_model.generate_path(\n",
    "    torch.tensor(test_maze.grid, dtype=torch.long).unsqueeze(0).to(device),\n",
    "    test_maze.start,\n",
    "    max_steps=100\n",
    ")\n",
    "\n",
    "print(f\"‚úì Simple RNN path: {len(rnn_path)} steps\")\n",
    "print(f\"‚úì LSTM path: {len(lstm_path)} steps\")\n",
    "print(f\"‚úì Optimal path: {len(optimal_solution)} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot each path\n",
    "test_maze_rnn = generate_simple_maze(size=15, seed=999)\n",
    "test_maze_lstm = generate_simple_maze(size=15, seed=999)\n",
    "test_maze_opt = generate_simple_maze(size=15, seed=999)\n",
    "\n",
    "# Mark paths\n",
    "for pos in rnn_path[1:-1]:\n",
    "    if test_maze_rnn.grid[pos] == 1:\n",
    "        test_maze_rnn.grid[pos] = 4\n",
    "\n",
    "for pos in lstm_path[1:-1]:\n",
    "    if test_maze_lstm.grid[pos] == 1:\n",
    "        test_maze_lstm.grid[pos] = 4\n",
    "\n",
    "# Restore start and goal\n",
    "test_maze_rnn.grid[test_maze.start] = 2\n",
    "test_maze_rnn.grid[test_maze.goal] = 3\n",
    "test_maze_lstm.grid[test_maze.start] = 2\n",
    "test_maze_lstm.grid[test_maze.goal] = 3\n",
    "\n",
    "MazeVisualizer.plot_maze(test_maze_rnn, ax=axes[0], \n",
    "                        title=f\"Simple RNN ({len(rnn_path)} steps)\")\n",
    "MazeVisualizer.plot_maze(test_maze_lstm, ax=axes[1], \n",
    "                        title=f\"LSTM ({len(lstm_path)} steps)\")\n",
    "MazeVisualizer.plot_maze(test_maze_opt, ax=axes[2], show_solution=True,\n",
    "                        title=f\"Optimal ({len(optimal_solution)} steps)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if reached goal\n",
    "rnn_reached = rnn_path[-1] == test_maze.goal\n",
    "lstm_reached = lstm_path[-1] == test_maze.goal\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Simple RNN: {'‚úì Reached goal' if rnn_reached else '‚úó Did not reach goal'}\")\n",
    "print(f\"  LSTM:       {'‚úì Reached goal' if lstm_reached else '‚úó Did not reach goal'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: The Critical Test - Performance vs Path Length\n",
    "\n",
    "### 7.1 Generate Mazes of Varying Difficulty\n",
    "\n",
    "**This is the key experiment**: How does performance degrade as paths get longer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on mazes with different path lengths\n",
    "length_bins = [0, 10, 15, 20, 25, 100]  # Path length ranges\n",
    "bin_labels = ['<10', '10-15', '15-20', '20-25', '25+']\n",
    "\n",
    "# Categorize validation mazes by path length\n",
    "binned_mazes = {label: [] for label in bin_labels}\n",
    "\n",
    "for idx, (maze, solution) in enumerate(val_dataset):\n",
    "    path_len = len(solution)\n",
    "    for i, (low, high) in enumerate(zip(length_bins[:-1], length_bins[1:])):\n",
    "        if low <= path_len < high:\n",
    "            binned_mazes[bin_labels[i]].append(idx)\n",
    "            break\n",
    "\n",
    "print(\"Mazes per difficulty bin:\")\n",
    "for label, indices in binned_mazes.items():\n",
    "    print(f\"  {label:6s}: {len(indices):2d} mazes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Test Models on Each Difficulty Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each model on each bin\n",
    "rnn_results = {label: [] for label in bin_labels}\n",
    "lstm_results = {label: [] for label in bin_labels}\n",
    "\n",
    "print(\"Testing models on different path lengths...\\n\")\n",
    "\n",
    "for label, indices in binned_mazes.items():\n",
    "    if len(indices) == 0:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Testing on {label} step mazes...\")\n",
    "    \n",
    "    for idx in indices:\n",
    "        maze, solution = val_dataset[idx]\n",
    "        maze_tensor = torch.tensor(maze.grid, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_path = simple_rnn.generate_path(maze_tensor, maze.start, max_steps=100)\n",
    "        rnn_success = (rnn_path[-1] == maze.goal)\n",
    "        rnn_results[label].append(rnn_success)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_path = lstm_model.generate_path(maze_tensor, maze.start, max_steps=100)\n",
    "        lstm_success = (lstm_path[-1] == maze.goal)\n",
    "        lstm_results[label].append(lstm_success)\n",
    "    \n",
    "    # Print results for this bin\n",
    "    rnn_acc = np.mean(rnn_results[label])\n",
    "    lstm_acc = np.mean(lstm_results[label])\n",
    "    print(f\"  RNN:  {rnn_acc:.2%} success rate\")\n",
    "    print(f\"  LSTM: {lstm_acc:.2%} success rate\\n\")\n",
    "\n",
    "print(\"‚úì Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualize the Sequential Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean accuracy per bin\n",
    "rnn_acc_by_length = [np.mean(rnn_results[label]) if rnn_results[label] else 0 \n",
    "                     for label in bin_labels]\n",
    "lstm_acc_by_length = [np.mean(lstm_results[label]) if lstm_results[label] else 0 \n",
    "                      for label in bin_labels]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(bin_labels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, rnn_acc_by_length, width, label='Simple RNN', \n",
    "               color='#E74C3C', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, lstm_acc_by_length, width, label='LSTM',\n",
    "               color='#3498DB', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.1%}',\n",
    "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Solution Path Length (steps)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Success Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('The Sequential Bottleneck: Performance Degrades with Path Length', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_labels)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observation:\")\n",
    "print(\"   As paths get longer, both RNN and LSTM struggle to maintain performance.\")\n",
    "print(\"   The 'memory backpack' can only hold so much!\")\n",
    "print(\"\\n   This is the fundamental limitation transformers solve!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Hidden State Analysis\n",
    "\n",
    "### 8.1 Visualize How Hidden State Evolves\n",
    "\n",
    "Let's look inside the \"memory backpack\" to see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a long maze\n",
    "long_maze_idx = None\n",
    "for idx, (maze, solution) in enumerate(val_dataset):\n",
    "    if len(solution) > 20:\n",
    "        long_maze_idx = idx\n",
    "        break\n",
    "\n",
    "if long_maze_idx is not None:\n",
    "    test_maze, test_solution = val_dataset[long_maze_idx]\n",
    "    print(f\"Analyzing maze with path length: {len(test_solution)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    maze_grid = torch.tensor(test_maze.grid, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positions = torch.tensor(test_solution, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass to get hidden states\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # We need to modify forward to return hidden states at each step\n",
    "        # For now, just show the concept\n",
    "        action_logits, (h_n, c_n) = lstm_model(maze_grid, positions)\n",
    "        \n",
    "    print(f\"\\n‚úì Hidden state analysis complete!\")\n",
    "    print(f\"   Final hidden state shape: {h_n.shape}\")\n",
    "    print(f\"   Final cell state shape: {c_n.shape}\")\n",
    "    \n",
    "    # Note: Full hidden state evolution visualization would require\n",
    "    # storing intermediate hidden states during forward pass\n",
    "    print(\"\\n   (Full hidden state evolution requires model modification)\")\n",
    "else:\n",
    "    print(\"No sufficiently long maze found in validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Sequential Processing is Limiting**\n",
    "   - RNNs must process inputs one at a time\n",
    "   - Information must flow through every intermediate state\n",
    "   - No shortcuts or \"teleportation\" between distant positions\n",
    "\n",
    "2. **Memory Degrades Over Distance**\n",
    "   - Both RNN and LSTM performance drops on longer paths\n",
    "   - The \"memory backpack\" has finite capacity\n",
    "   - Early information gets compressed/forgotten\n",
    "\n",
    "3. **LSTM Helps But Doesn't Solve the Problem**\n",
    "   - Gates allow better gradient flow\n",
    "   - Cell state provides a \"memory highway\"\n",
    "   - But still fundamentally sequential\n",
    "\n",
    "### The Question for Next Time\n",
    "\n",
    "**What if we could see the entire maze at once?**\n",
    "\n",
    "Instead of processing step-by-step like a mouse:\n",
    "- üó∫Ô∏è View from above (bird's-eye perspective)\n",
    "- üîó Connect any two positions directly\n",
    "- ‚ö° Process all positions in parallel\n",
    "\n",
    "This is what **attention** enables. In Notebook 2, we'll build this mechanism from scratch!\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises (Optional)\n",
    "\n",
    "1. **Experiment with hidden dimensions**: Try hidden_dim=64 and hidden_dim=256. How does this affect performance?\n",
    "\n",
    "2. **Add GRU**: Implement a GRU variant and compare to RNN and LSTM\n",
    "\n",
    "3. **Longer mazes**: Generate 20√ó20 mazes. How much does performance degrade?\n",
    "\n",
    "4. **Attention preview**: Can you think of how to let position 20 \"see\" position 1 directly?\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Notebook 2 - The Map (Attention)\n",
    "\n",
    "We'll implement attention from scratch and see how it transforms this sequential bottleneck into a parallel process!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
