# Transformer Maze: Complete Learning Path Overview

## The Journey from Sequential to Continuous

This document provides a comprehensive overview of the entire Transformer Maze learning experience. Think of it as your roadmap through the complete evolution of sequence modeling, from classical RNNs to modern transformers to cutting-edge continuous approaches.

---

## Table of Contents

- [The Core Narrative](#the-core-narrative)
- [Why Mazes?](#why-mazes)
- [The Six-Notebook Arc](#the-six-notebook-arc)
- [Key Mathematical Concepts](#key-mathematical-concepts)
- [The Canadian Connection](#the-canadian-connection)
- [Prerequisites and Learning Path](#prerequisites-and-learning-path)
- [What You'll Understand After Completion](#what-youll-understand-after-completion)
- [Beyond This Course](#beyond-this-course)

---
![Mazes](../img/Mazes-RNNs-vs-Transformer.png)

---
> There's no need to build a labyrinth when the entire universe is one.   
> ‚Äî Jorge Luis Borges, *Ibn-Hakim Al-Bokhari, Murdered in His Labyrinth*

> *‚ÄúAh no,‚Äù he said, ‚ÄúI see the source of the misunderstanding now. No, look, you see what happened was that we used to do experiments on them. They were often used in behavioral research, Pavlov and all that sort of stuff. So what happened was that the mice would be set all sorts of tests, learning to ring bells, run round mazes and things so that the whole nature of the learning process could be examined. From our observations of their behavior we were able to learn all sorts of things about our own ‚Ä¶‚Äù Arthur‚Äôs voice trailed off.   
‚ÄúSuch subtlety ‚Ä¶‚Äù said Slartibartfast, ‚Äúone has to admire it.‚Äù    
‚ÄúWhat?‚Äù said Arthur.    
‚ÄúHow better to disguise their real natures, and how better to guide your thinking. Suddenly running down a maze the wrong way, eating the wrong bit of cheese, unexpectedly dropping dead of myxomatosis. If it‚Äôs finely calculated the cumulative effect is enormous.‚Äù He paused for effect. ‚ÄúYou see, Earthman, they really are particularly clever hyper-intelligent pandimensional beings. Your planet and people have formed the matrix of an organic computer running a ten-million-year research program‚Ä¶. Let me tell you the whole story.‚Äù*.  
‚Äï Douglas Adams, *The Hitchhiker's Guide to the Galaxy*
---


## The Core Narrative

### The Problem: Understanding Sequences

At its heart, modern AI is about understanding and generating sequences. Whether it's words in a sentence, pixels in an image, or moves through a maze, the fundamental challenge is the same: **how do we capture dependencies between elements that might be far apart?**

This course traces the evolution of solutions to this problem:

```
Sequential Processing (RNNs)
    ‚Üì
Parallel Processing (Attention/Transformers)
    ‚Üì
Continuous Processing (Neural ODEs)
    ‚Üì
Hybrid Future (Combining strengths)
```

### The Three Paradigm Shifts

**SHIFT 1: From Sequential to Parallel (2014-2017)**
- Problem: RNNs process one step at a time
- Solution: Attention mechanism allows parallel processing
- Impact: BERT, GPT, and the LLM revolution

**SHIFT 2: From Fixed to Adaptive (2018)**
- Problem: Fixed depth networks waste computation
- Solution: Neural ODEs enable adaptive computation
- Impact: New research direction, memory efficiency

**SHIFT 3: From Pure Attention to Hybrid (2023-2025)**
- Problem: Transformers have quadratic complexity
- Solution: State space models, sparse patterns, hybrid approaches
- Impact: Current frontier - models like Mamba, long context windows

---

## Why Mazes?

Mazes are the PERFECT teaching tool for understanding sequence models. Here's why:

### 1. Historical Significance
- Claude Shannon's mechanical mouse (1950) - the first learning machine
- Bellman's dynamic programming (1957) - optimal path finding
- A-star search (1968) - heuristic search
- Q-learning demos (1989) - reinforcement learning
- Modern benchmark for navigation agents

### 2. Core AI Challenges in Miniature
- **Long-range dependencies**: Start connects to goal across many steps
- **Spatial reasoning**: Understanding 2D structure
- **Sequential decision making**: Each move affects future options
- **Credit assignment**: Which early moves enabled the solution?
- **State space exploration**: Multiple possible paths

### 3. Visualization Clarity
- You can SEE what the model learned
- Attention patterns map directly to spatial reasoning
- Success/failure is unambiguous
- Easy to generate infinite training data
- Difficulty is tunable (size, wall density)

### 4. Computational Tractability
- Small enough to train quickly
- Large enough to be non-trivial
- Scales from 5x5 (easy) to 50x50 (hard)
- Enables rapid experimentation

### 5. Real-World Relevance
Maze-solving skills transfer to:
- Robot navigation
- Game playing (Go, Chess)
- Circuit design
- Network routing
- Molecular pathway finding
- Code navigation

---

## The Six-Notebook Arc

### Notebook 1: The Mouse - RNN Sequential Processing

**Core Metaphor**: A mouse navigating by following its nose

**What You Learn**:
- How RNNs process sequences one step at a time
- Why this creates a sequential bottleneck
- The vanishing gradient problem in practice
- LSTM improvements (and limitations)

**Key Experiment**: 
Performance vs path length - demonstrating that RNNs get worse as sequences get longer. This is the FUNDAMENTAL LIMITATION that motivates everything that follows.

**Mathematical Foundation**:
```
h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)
```
Information must flow through ALL intermediate states.

**Aha Moment**: "Wait, why can't the model just SEE the goal directly?"

**Time**: 45-60 minutes  
**Difficulty**: Beginner  
**Prerequisites**: Basic Python, basic neural networks

---

### Notebook 2: The Map - Attention Fundamentals

**Core Metaphor**: Looking at a map from above vs walking through maze

**What You Learn**:
- Attention as a differentiable lookup mechanism
- Query, Key, Value - the database analogy
- Why we scale by sqrt(d_k)
- How attention enables "teleportation" across sequences
- Multi-head attention - multiple perspectives

**Key Experiment**:
Demonstrating START‚ÜíGOAL attention weight. The model creates a DIRECT CONNECTION across the entire maze, bypassing the sequential bottleneck.

**Mathematical Foundation**:
```
Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V
```
Why the scaling? We prove it prevents gradient saturation.

**Aha Moment**: "Transformers turn temporal problems into spatial problems!"

**Time**: 60-75 minutes  
**Difficulty**: Intermediate  
**Prerequisites**: Notebook 1, linear algebra, softmax

---

### Notebook 3: Building Transformer Blocks

**Core Metaphor**: Assembling a complete system from components

**What You Learn**:
- Positional encoding - why and how
- Feed-forward networks - processing power
- Residual connections - gradient highways
- Layer normalization - training stability
- How all pieces fit together

**Key Experiments**:
- Shuffled sequences showing need for position
- Gradient flow through 50+ layers
- Complete training loop with real performance metrics

**Mathematical Foundations**:
```
Positional Encoding: PE(pos,2i) = sin(pos/10000^(2i/d))
Residual: h_{l+1} = h_l + f(h_l)
LayerNorm: (x - Œº) / sqrt(œÉ¬≤ + Œµ)
```

**Aha Moment**: "Oh! Residuals solve the vanishing gradient problem - that's why we can go deep!"

**Time**: 60-90 minutes  
**Difficulty**: Intermediate  
**Prerequisites**: Notebooks 1-2, backpropagation understanding

---

### Notebook 4: Full Transformer Architectures

**Core Metaphor**: Three ways to use the same building blocks

**What You Learn**:
- Encoder-only (BERT) - understanding and classification
- Decoder-only (GPT) - generation and completion
- Encoder-decoder (T5) - transformation tasks
- Causal masking - preventing future information leakage
- Cross-attention - bridging encoder and decoder
- **Why decoder-only won** - the modern LLM architecture

**Key Experiments**:
- Training all three architectures on maze tasks
- Visualizing causal masks
- Comparing parameter efficiency
- Autoregressive generation

**Mathematical Foundations**:
```
Causal Mask: mask[i,j] = 1 if j ‚â§ i else 0
Cross-Attention: Q from decoder, K,V from encoder
```

**Aha Moment**: "Decoder-only can do everything via prompting - that's why GPT, Claude, and LLaMA all chose this!"

**Time**: 90-120 minutes  
**Difficulty**: Advanced  
**Prerequisites**: Notebooks 1-3, understanding of different NLP tasks

---

### Notebook 5: Modern Transformer Variants

**Core Metaphor**: Optimization and efficiency at scale

**What You Learn**:
- **The quadratic problem**: Why O(n¬≤) matters
- **Sparse attention**: Local, strided, block patterns
- **RoPE**: Rotary position embeddings for better extrapolation
- **Flash Attention**: Algorithmic optimization (2-4x faster)
- **State space models**: Linear complexity alternative (Mamba)
- Trade-offs: efficiency vs expressiveness

**Key Experiments**:
- Memory usage scaling to 8K+ sequences
- Sparse attention pattern comparison
- RoPE vs sinusoidal length extrapolation
- SSM vs Transformer complexity analysis

**Mathematical Foundations**:
```
Standard Attention: O(n¬≤ ¬∑ d) memory and compute
Sparse Attention: O(n ¬∑ k ¬∑ d) where k << n
RoPE: Rotation matrices for relative positions
SSM: h_t = Ah_{t-1} + Bx_t, y_t = Ch_t
```

**Aha Moment**: "The field is moving beyond pure attention - Mamba shows you don't need quadratic complexity!"

**Time**: 75-90 minutes  
**Difficulty**: Advanced  
**Prerequisites**: Notebooks 1-4, complexity analysis

---

### Notebook 6: Neural ODEs and Continuous Dynamics üá®üá¶

**Core Metaphor**: From stepping stones to flowing water

**What You Learn**:
- **ResNets as Euler method**: Discrete ‚Üí continuous connection
- **Neural ODEs**: Continuous-depth networks
- **Adaptive computation**: Use more depth when needed
- **Memory efficiency**: Adjoint method = constant memory
- **Toronto's legacy**: Hinton ‚Üí Bahdanau ‚Üí Duvenaud
- **Hybrid future**: Combining discrete and continuous

**Key Experiments**:
- Visualizing continuous trajectories in state space
- Comparing memory: ResNet (linear) vs Neural ODE (constant)
- Adaptive depth - different examples need different computation
- Hybrid transformer blocks

**Mathematical Foundations**:
```
Discrete: h_{l+1} = h_l + f(h_l)
Continuous: dh/dt = f(h,t)
Forward: h(1) = h(0) + ‚à´‚ÇÄ¬π f(h(t),t) dt
Backward: Adjoint method (constant memory)
```

**Aha Moment**: "Transformers are just discrete samplings of a continuous process - we can make depth adaptive!"

**Time**: 75-90 minutes  
**Difficulty**: Advanced  
**Prerequisites**: All previous notebooks, basic differential equations

---

## Key Mathematical Concepts

### The Progressive Mathematical Journey

**Notebook 1: Sequential Recursion**
- Hidden state updates: h_t = f(h_{t-1}, x_t)
- Backpropagation through time
- Vanishing/exploding gradients

**Notebook 2: Matrix Operations**
- Dot products as similarity: QK^T
- Softmax as differentiable argmax
- Weighted sums as differentiable lookup

**Notebook 3: Architectural Components**
- Sinusoidal functions for position
- Residual connections as gradient highways
- Normalization for training stability

**Notebook 4: Attention Variants**
- Masking for causality
- Cross-attention for alignment
- Architectural choices and trade-offs

**Notebook 5: Complexity Analysis**
- Big-O notation in practice
- Memory vs compute trade-offs
- Sparsity and approximation

**Notebook 6: Continuous Mathematics**
- Ordinary differential equations
- Euler's method and ODE solvers
- Adjoint sensitivity method
- Dynamical systems perspective

### No PhD Required

Each concept is:
- Introduced with intuition first
- Illustrated with visualizations
- Implemented in working code
- Connected to practical impact

If you can understand derivatives and matrix multiplication, you can follow this course.

---

## The Canadian Connection üá®üá¶

### University of Toronto's Deep Learning Legacy

This course deliberately highlights Canada's foundational contributions to modern AI:

**1986: Backpropagation**
- Rumelhart, Hinton, Williams
- Made training deep networks feasible
- Foundation of all modern deep learning

**2006: Deep Belief Networks**
- Hinton, Osindero, Teh
- Reignited neural network research
- Proved deep learning could work

**2012: AlexNet**
- Krizhevsky, Sutskever, Hinton
- ImageNet breakthrough
- Started the deep learning revolution

**2014: Attention Mechanism**
- Bahdanau, Cho, Bengio (Montreal/Toronto collaboration)
- Foundation for transformers
- Enabled machine translation breakthrough

**2018: Neural ODEs**
- Chen, Rubanova, Bettencourt, Duvenaud
- **NeurIPS 2018 Best Paper**
- Continuous-depth networks
- Memory-efficient training

### Why This Matters

These aren't just historical footnotes - they represent fundamental shifts in how we think about learning:

- **Hinton**: Made deep networks trainable
- **Bahdanau**: Made them see the whole picture
- **Duvenaud**: Made depth continuous and adaptive

The progression from **discrete layers** (Hinton) ‚Üí **attention mechanisms** (Bahdanau) ‚Üí **continuous dynamics** (Duvenaud) mirrors the structure of this course.

### Toronto's Ongoing Impact

Current research continues:
- Vector Institute (founded 2017)
- Geoffrey Hinton (Turing Award 2018)
- Ongoing work in efficient transformers, adaptive computation
- Training next generation of AI researchers

---

## Prerequisites and Learning Path

### What You Need to Start

**Essential Background**:
- Python programming (intermediate level)
- Basic neural networks (what's a layer, activation function)
- Linear algebra (matrix multiplication, vectors)
- Calculus (derivatives, chain rule)

**Helpful But Not Required**:
- PyTorch experience
- Backpropagation understanding
- Previous NLP/sequence modeling
- Differential equations (only for Notebook 6)

### Recommended Learning Path

**Option 1: Complete Beginner Path** (20-25 hours)
1. Review neural network basics
2. Notebook 1 (with extra time for concepts)
3. Linear algebra refresher if needed
4. Notebook 2
5. Notebook 3
6. Pause - implement your own simple transformer
7. Notebook 4
8. Notebook 5 (can skim some sections)
9. Notebook 6 (conceptual understanding fine)

**Option 2: Experienced Practitioner Path** (10-15 hours)
1. Skim Notebook 1 (you know RNNs)
2. Notebook 2 carefully (attention details matter)
3. Notebook 3 (how pieces fit)
4. Notebook 4 (architecture comparison valuable)
5. Notebook 5 (modern techniques you need)
6. Notebook 6 (cutting edge perspective)

**Option 3: Research/Advanced Path** (8-12 hours)
1. Notebook 2 (attention mathematics)
2. Notebook 3 (architectural principles)
3. Skim Notebook 4 (you know the architectures)
4. Notebook 5 carefully (modern frontier)
5. Notebook 6 carefully (research directions)
6. Explore papers in references

### Time Estimates

- **Quick overview**: 2-3 hours (run all notebooks, read key sections)
- **Solid understanding**: 12-15 hours (complete all notebooks, experiments)
- **Deep mastery**: 20-30 hours (implement extensions, explore papers)
- **Teaching prep**: 30-40 hours (understand every detail, prepare materials)

### Learning Checkpoints

After each notebook, you should be able to:

**Notebook 1**:
- ‚úì Explain why RNNs struggle with long sequences
- ‚úì Implement an LSTM from scratch
- ‚úì Describe vanishing gradient problem

**Notebook 2**:
- ‚úì Derive the attention equation
- ‚úì Explain Query, Key, Value roles
- ‚úì Implement scaled dot-product attention

**Notebook 3**:
- ‚úì Build a complete transformer encoder
- ‚úì Explain why each component is necessary
- ‚úì Train a model on a real task

**Notebook 4**:
- ‚úì Compare encoder-only, decoder-only, encoder-decoder
- ‚úì Implement causal masking
- ‚úì Explain why decoder-only dominates LLMs

**Notebook 5**:
- ‚úì Analyze complexity of attention variants
- ‚úì Explain RoPE advantages
- ‚úì Compare transformers to SSMs

**Notebook 6**:
- ‚úì Connect ResNets to ODEs
- ‚úì Explain adaptive computation
- ‚úì Describe Toronto's contributions

---

## What You'll Understand After Completion

### Technical Understanding

**Architecture Mastery**:
- Complete transformer implementation from scratch
- All three main architectures (encoder/decoder/enc-dec)
- Modern variants (sparse, RoPE, Flash Attention)
- Alternative paradigms (SSMs, Neural ODEs)

**Mathematical Foundations**:
- Why attention works (not just how)
- Complexity analysis and trade-offs
- Gradient flow and training stability
- Continuous vs discrete perspectives

**Practical Skills**:
- Debug attention patterns
- Choose right architecture for task
- Optimize for memory or speed
- Implement modern improvements

### Conceptual Understanding

**The Big Picture**:
- Why transformers triggered LLM revolution
- Where the field is headed (hybrid models, adaptive computation)
- Trade-offs between different approaches
- How research builds on previous work

**Historical Context**:
- Evolution from RNNs to transformers to Neural ODEs
- Key breakthroughs and who made them
- Canada's role in shaping modern AI
- Why certain approaches won out

**Future Directions**:
- Current research frontiers
- Open problems (context length, efficiency)
- Promising new directions (Mamba, hybrid models)
- How to stay current with rapid changes

### Career Impact

**For Job Seekers**:
- Portfolio piece demonstrating deep understanding
- Can explain transformers from first principles
- Hands-on implementation experience
- Understanding of modern variants

**For Researchers**:
- Solid foundation for reading papers
- Implementation skills for experiments
- Understanding of historical context
- Framework for evaluating new approaches

**For Engineers**:
- Know when to use which architecture
- Can debug model performance issues
- Understand memory/compute trade-offs
- Can implement optimizations

**For Educators**:
- Complete teaching materials
- Progressive learning structure
- Working code examples
- Historical and modern context

---

## Beyond This Course

### Next Steps for Different Goals

**If You Want to Build Production LLMs**:
1. Study: Scaling laws, distributed training
2. Read: GPT-3 paper, LLaMA technical report
3. Explore: Hugging Face Transformers library
4. Implement: Fine-tuning with LoRA/QLoRA
5. Project: Build domain-specific model

**If You Want to Do Research**:
1. Deep dive: Papers on efficient transformers
2. Explore: Mamba, RWKV, other alternatives
3. Implement: Recent papers from scratch
4. Experiment: Your own architectural ideas
5. Contribute: Open source implementations

**If You Want to Understand AI Better**:
1. Broaden: Computer vision (ViT, CLIP)
2. Explore: Reinforcement learning (Decision Transformer)
3. Learn: Diffusion models, flow matching
4. Connect: How transformers fit in bigger picture
5. Teach: Explain concepts to others

**If You Want to Optimize Models**:
1. Study: Flash Attention paper in detail
2. Learn: CUDA programming, kernel fusion
3. Explore: Quantization, pruning, distillation
4. Profile: Real model performance
5. Optimize: Implement custom kernels

### Recommended Reading

**Papers** (in order of difficulty):
1. "Attention Is All You Need" (Vaswani et al., 2017)
2. "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al., 2018)
3. "Language Models are Few-Shot Learners" (GPT-3, Brown et al., 2020)
4. "Neural Ordinary Differential Equations" (Chen et al., 2018)
5. "Flash Attention" (Dao et al., 2022)
6. "Mamba: Linear-Time Sequence Modeling" (Gu & Dao, 2023)

**Books**:
- "Deep Learning" by Goodfellow, Bengio, Courville
- "Understanding Deep Learning" by Simon Prince (free online)
- "Natural Language Processing with Transformers" by Tunstall et al.

**Online Resources**:
- The Annotated Transformer (Harvard NLP)
- Jay Alammar's blog (visual explanations)
- Hugging Face course (practical applications)
- Papers with Code (latest research)

### Open Problems to Explore

**Efficiency**:
- Context length beyond 1M tokens
- Sub-linear attention mechanisms
- Memory-efficient training at scale

**Architecture**:
- Optimal hybrid discrete-continuous models
- Task-adaptive architectures
- Self-modifying networks

**Understanding**:
- Why transformers work so well
- Interpretability of attention patterns
- Theoretical foundations

**Applications**:
- Multi-modal transformers (text, vision, audio)
- Scientific computing with transformers
- Hardware-software co-design

---

## How to Use This Material

### As a Course

**Week 1-2**: Notebooks 1-2 (Sequential to Parallel)
- Understand the fundamental problem
- See how attention solves it
- Assignment: Implement attention from scratch

**Week 3-4**: Notebooks 3-4 (Complete Architectures)
- Build complete transformers
- Compare three paradigms
- Assignment: Train model on your task

**Week 5-6**: Notebooks 5-6 (Modern and Future)
- Explore cutting edge
- Understand research directions
- Assignment: Implement modern variant

**Final Project**: 
- Extend to new domain
- Implement recent paper
- Propose novel architecture

### As Self-Study

**Phase 1: Foundation** (First pass)
- Run all notebooks
- Understand main concepts
- Don't worry about every detail

**Phase 2: Depth** (Second pass)
- Implement components yourself
- Experiment with variations
- Read referenced papers

**Phase 3: Mastery** (Third pass)
- Teach concepts to someone else
- Build something novel
- Contribute improvements

### As Reference Material

**Quick Lookup**:
- Use QUICK_REFERENCE.md for code patterns
- Check specific notebooks for concepts
- Browse visualizations for intuition

**Deep Dive**:
- Full mathematical derivations in notebooks
- Working code for every concept
- References to original papers

**Teaching Resource**:
- Complete worked examples
- Progressive complexity
- Multiple explanations of same concept

---

## Project Philosophy

### Why We Built This

**Problem**: Most transformer tutorials either:
1. Are too high-level ("just use Hugging Face")
2. Are too math-heavy (dense papers)
3. Skip the intuition (straight to code)
4. Miss the historical context
5. Don't cover modern variants

**Solution**: A complete journey that:
- Builds intuition with mazes
- Implements everything from scratch
- Explains the "why" not just "how"
- Connects past to present to future
- Honors Canadian contributions

### Design Principles

**Progressive Complexity**:
- Each notebook builds on previous
- Never introduce two hard things at once
- Repeat key concepts in different contexts

**Visual First**:
- Every concept has a visualization
- Attention patterns you can see
- Performance metrics you can track

**Working Code**:
- Nothing is pseudo-code
- All examples actually run
- You can modify and experiment

**Multiple Explanations**:
- Intuition (metaphors)
- Mathematics (equations)
- Code (implementation)
- Visualization (seeing is understanding)

**Historical Grounding**:
- Why problems arose
- Who solved them
- How solutions built on each other

**Forward Looking**:
- Current research frontiers
- Open problems
- Where to go next

---

## Community and Contribution

### This Is Open Source

- MIT License - use freely
- Contributions welcome
- Issues and PRs encouraged
- Educational use explicitly supported

### Ways to Contribute

**For Beginners**:
- Report unclear explanations
- Suggest additional visualizations
- Share your learning journey
- Ask questions (they help everyone)

**For Practitioners**:
- Add more datasets/tasks
- Implement paper extensions
- Improve efficiency
- Add more architectures

**For Researchers**:
- Add cutting-edge variants
- Connect to recent papers
- Improve mathematical exposition
- Extend to other domains

**For Educators**:
- Classroom-tested improvements
- Additional exercises
- Assessment materials
- Translation to other languages

### Contact and Support

- GitHub Issues: Bug reports, questions
- Discussions: Ideas, extensions
- Pull Requests: Improvements, additions
- Twitter/LinkedIn: Share your experience

---

## Acknowledgments

### Standing on Giants' Shoulders

This work synthesizes contributions from:

**Original Researchers**:
- Geoffrey Hinton and collaborators (backpropagation, deep learning)
- Dzmitry Bahdanau and collaborators (attention mechanism)
- David Duvenaud and collaborators (Neural ODEs)
- Vaswani et al. (transformer architecture)
- The entire deep learning community

**Educational Inspirations**:
- Andrej Karpathy's "Zero to Hero" series
- Jay Alammar's visual explanations
- The Annotated Transformer
- Fast.ai's teaching philosophy

**Open Source Tools**:
- PyTorch (Meta AI)
- NumPy (community)
- Matplotlib (community)
- Jupyter (community)

### The Maze Metaphor

Special recognition to Claude Shannon, whose 1950 mechanical mouse "Theseus" showed that machines could learn. Seven decades later, we're still using mazes to understand intelligence.

---

## Final Thoughts

### The Journey Matters

You could skip straight to using GPT-4 API. But then you'd miss:
- **Why** transformers work
- **When** to use which architecture
- **How** to debug when things break
- **Where** the field is heading

Understanding the fundamentals makes you a better practitioner, researcher, and engineer.

### The Canadian Thread

From Hinton's backpropagation (making networks trainable) to Bahdanau's attention (making them powerful) to Duvenaud's Neural ODEs (making them efficient), Canada has shaped modern AI at every step.

This course honors that legacy while looking forward to what's next.

### What's Next for You?

After completing this course, you won't just know how to use transformers - you'll understand them deeply enough to:
- Debug them when they fail
- Optimize them for your needs
- Modify them for new tasks
- Propose improvements
- Follow cutting-edge research

Most importantly, you'll have **intuition** - that sense of how these models think, what they can and can't do, and where the field is heading.

### Go Build Something!

The best way to solidify your understanding is to build. Some ideas:

- Train a transformer on a domain you care about
- Implement a recent paper from scratch
- Optimize for your specific constraints
- Teach someone else what you learned
- Propose a novel architecture

The tools are here. The knowledge is transferable. The future is unwritten.

**Welcome to the journey from sequential to continuous, from RNNs to Neural ODEs, from understanding to mastery.**

Now go solve some mazes - both literal and metaphorical! üöÄ

---

*Last updated: November 2025*  
*For the latest version, visit: https://github.com/roguetrainer/transformer-maze*